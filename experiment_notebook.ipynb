{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We opt to define all the global configuration settings through this dict\n",
    "config = {\"resampling_fs\" : 22050, #expected resampling rate on HDF5 file creation\n",
    "          \"dst_path\":\"./choral_dst\", # User Dataset path (if None, the default one will be used)\n",
    "          \"hdf5_filepath\":\"./choral_dst.hdf5\",\n",
    "          \"batch_size_train\": 2048,\n",
    "          \"batch_size_test\": 512,\n",
    "          \"num_frames\": 132300 # 8 sec @ 22050Hz\n",
    "         }\n",
    "\n",
    "if not os.path.isdir('./debug'):\n",
    "    os.mkdir('./debug')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Data Handling Methodology</center></h1>\n",
    "\n",
    "For the first step of this task, we will work from the __Choral Singing Dataset__, which contains three choir songs composed of 4 groups of 4 singers (16 parts per song), for a total of 48 stems. The dataset can be downloaded from Zenodo at the following:\n",
    "\n",
    "https://zenodo.org/record/2649950#.XlJRNy2ZPRY\n",
    "\n",
    "If you happen to already have the dataset locally, please define it's local path through the ***dst_path*** variable. Otherwise, the cell below will take care of downloading it for you:\n",
    "***\n",
    "### 1. Dataset Download:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell downloads the Mridangam Stroke Dataset in its entirety\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import os, sys,shutil\n",
    "\n",
    "#Link for the dataset. If you already have the dataset locally, define its location through the config dict above \n",
    "url = 'https://zenodo.org/record/2649950/files/ChoralSingingDataset.zip'\n",
    "\n",
    "if config[\"dst_path\"] == None:\n",
    "    main_data_dir = './choral_dst'\n",
    "    if not os.path.exists(main_data_dir): #creating the directory if not exist\n",
    "        os.mkdir(main_data_dir)\n",
    "    print('Downloading Choral Singing Dataset Dataset...')\n",
    "    foldername = url.split('/')[-1]\n",
    "    urllib.request.urlretrieve(url,foldername)\n",
    "    #Unzipping to a specific folder\n",
    "    zip_ref = zipfile.ZipFile(foldername, 'r')\n",
    "    zip_ref.extractall(main_data_dir)\n",
    "    zip_ref.close()\n",
    "    os.remove(foldername)#Removing the zip file\n",
    "    print('Data downloaded and unzipped to: ',main_data_dir)\n",
    "else:\n",
    "    print('User Path Defined. Dataset Expected to be Found at the Following:\\n'+config[\"dst_path\"])\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 2. Storing our Data in a Convenient Format\n",
    "\n",
    "For the sake of simplicity and clarity, we will store our song stems in a **HDF5 file**, which will help us to manage the later steps a bit more easily. This hierarchical file format will allow us to store our data in an intuitive way as follow:\n",
    "\n",
    "**[HDF5_File] -> [Set] -> [Song] -> [Part]**\n",
    "\n",
    "Since our dataset is of *very* limited size, we will break it down as follow:\n",
    "\n",
    "| Set   | Stems | Songs |\n",
    "| :---:   | :---: | :---: |\n",
    "| Train |    24   |    A, B   |\n",
    "| Valid |   8    |   A, B    |\n",
    "| Test  |16| C |\n",
    "\n",
    "**Note:** By default, the Choral Singing Dataset follow the following naming convention:\n",
    "\n",
    "*DatasetName_SongAcronym_PartName_PartNumber.wav*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import librosa\n",
    "import soundfile\n",
    "import glob\n",
    "import random\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from time import sleep\n",
    "from pathlib import Path\n",
    "\n",
    "# First check if HDF5 already exists:\n",
    "hdf5_file = Path(config['hdf5_filepath'])\n",
    "if not hdf5_file.is_file():\n",
    "    tracks = dict()\n",
    "    resampling_fs = config['resampling_fs']\n",
    "    dst_path = './choral_dst' if (config['dst_path'] == None) else config['dst_path']\n",
    "\n",
    "    # Get all the stem files\n",
    "    dst_stems = glob.glob(dst_path+\"/*.wav\",recursive=False)\n",
    "    # Extract only songs from name\n",
    "    songs = set([os.path.splitext(os.path.basename(i))[0].split('_')[1] for i in dst_stems])\n",
    "    # Take 2/3 of songs for train/valid\n",
    "    train_val_songs = random.sample(songs, int(np.ceil(0.666*len(songs))))\n",
    "    # Take the rest as test set\n",
    "    test_song = list(set(songs) - set(train_val_songs))\n",
    "    test_idx = [int(i) for i, track in enumerate(dst_stems) if [True for song in test_song if str('_'+song+'_') in track]]\n",
    "    # Take one random group of singer per song for each song as validation set\n",
    "    rand_part = random.randint(1,4)\n",
    "    val_idx   = [int(i) for i, track in enumerate(dst_stems) if [True for song in train_val_songs if ((str('_'+song+'_') in track) and (str('_'+str(rand_part)+'.wav')) in track)]]\n",
    "    # Deduce the train set from valid / test set\n",
    "    train_idx = list(set(range(len(dst_stems))) -  (set(val_idx + test_idx)))\n",
    "\n",
    "    # tracks['train'] = np.take(dst_stems, train_idx)\n",
    "    tracks['train'] = np.take(dst_stems, train_idx)\n",
    "    tracks['valid'] = np.take(dst_stems, val_idx)\n",
    "    tracks['test'] = np.take(dst_stems, test_idx)\n",
    "\n",
    "    print('Ready to create HDF5')\n",
    "\n",
    "    h5file = h5py.File(config[\"hdf5_filepath\"], \"w\")\n",
    "\n",
    "    # Write stems to hdf5 file for train/valid/test partitions\n",
    "    for curr_partition in [\"train\", \"valid\", \"test\"]:\n",
    "\n",
    "        print(\"Writing \" + curr_partition + \" partition with \"+str(len(tracks[curr_partition]))+\" files...\")\n",
    "\n",
    "        stem_list = tracks[curr_partition]\n",
    "\n",
    "        # Create group for set if needed\n",
    "        if not str(curr_partition) in h5file:\n",
    "            set_grp  = h5file.create_group(curr_partition)\n",
    "\n",
    "        sleep(1)\n",
    "\n",
    "        for track in tqdm(stem_list):\n",
    "\n",
    "            filename = os.path.splitext(os.path.basename(track))[0].split('_')\n",
    "\n",
    "            song = filename[1]\n",
    "            part = ''.join(filename[2:4])\n",
    "\n",
    "            # Create group for the Song if needed\n",
    "            if not str(curr_partition+'/'+song) in h5file:\n",
    "                song_grp  = set_grp.create_group(song)\n",
    "\n",
    "            # Create group for the Part if needed\n",
    "            if not str(curr_partition+'/'+song+'/'+part) in h5file:\n",
    "                part_grp = h5file[str(curr_partition+'/'+song)]\n",
    "                subgrp  = part_grp.create_group(part)\n",
    "\n",
    "            # Once part groups / song subgroups are created, store file\n",
    "            audio, s = librosa.load(track, sr=resampling_fs)\n",
    "            subgrp = h5file[str(curr_partition+'/'+song+'/'+part)]\n",
    "            subgrp.create_dataset(\"raw_wav\",data=audio)\n",
    "\n",
    "    print('Done Creating HDF5 at path: '+config['hdf5_filepath'])\n",
    "    h5file.close()\n",
    "else:\n",
    "    print('HDF5 Already Exists. File Expected to be Found at the Following:\\n'+config[\"hdf5_filepath\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 3. Defining the Generator Function:\n",
    "\n",
    "Instead of computing features on the entirety of each of these songs, we will create many song snippets (or frames), which will depict different possible musical settings (SAT, ATB, SB, etc.) out of the songs. This will allow us to cover all possible singer permutations on a per-frame basis, without having to compute and feed to the model substantial chunk of audio.\n",
    "\n",
    "The next step would be to define the function that will generate these song frames for us. \n",
    "\n",
    "**Note:** This function will yield the song segments but won't render them to disk. We can proceed by computing everything on the fly and save ourselves some local memory space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchGenerator(hdf5_filepath, batch_size, num_frames, partition='train', debug=False):\n",
    "    \n",
    "    dataset   = h5py.File(hdf5_filepath, \"r\")\n",
    "    itCounter = 0\n",
    "\n",
    "    itCounter = itCounter + 1\n",
    "    \n",
    "    # Choose random song and retrieve all available part for it\n",
    "    randsong = random.choice(list(dataset[partition].keys())) # Get random song from dataset keys\n",
    "    sources  = list(dataset[partition][randsong].keys())      # Get all available sources for song\n",
    "    \n",
    "    # Compute available parts per group\n",
    "    part_per_group = dict({'soprano':None,'alto':None,'tenor':None,'bass':None})\n",
    "    for group in part_per_group.keys():\n",
    "        parts = [source[-1] for source in sources if group in source]\n",
    "        part_per_group[group] = parts\n",
    "\n",
    "    # Init the start/end values of song snippet\n",
    "    startspl = 0\n",
    "    endspl   = 0\n",
    "\n",
    "    # Preping the output shape of the generator\n",
    "    out_shapes = {'mix':np.zeros((batch_size, num_frames)),'num_sing':np.zeros((batch_size),dtype=int)}\n",
    "\n",
    "    # Will create batch_size x mixes of unique singers permutations\n",
    "    for i in range(batch_size):\n",
    "\n",
    "        randsources = random.sample(part_per_group.keys(), random.randint(1,4)) # Randomize source pick\n",
    "\n",
    "        # Get Start and End samples. Pick random part to calculate start/end spl\n",
    "        randpart = random.choice(sources)\n",
    "        startspl = random.randint(0,len(dataset[partition][randsong][randpart]['raw_wav'])-num_frames) # This assume that all stems are the same length\n",
    "        endspl   = startspl+num_frames\n",
    "\n",
    "        # Get Random Sources with part number: \n",
    "        randsources_with_part = [\"{}{}\".format(a, random.choice(part_per_group[a])) for a in randsources] # Concatenate strings for part name\n",
    "        # Retrieve the chunks and store them in output shapes                                         \n",
    "        for source in randsources_with_part:\n",
    "\n",
    "            source_chunk = dataset[partition][randsong][source]['raw_wav'][startspl:endspl]              # Retrieve part's chunk\n",
    "            out_shapes['mix'][i] = np.add(out_shapes['mix'][i],source_chunk)            # Add the chunk to the mix\n",
    "\n",
    "        # Scale down the mix based off number of sources\n",
    "        scaler = len(randsources_with_part)\n",
    "        out_shapes['mix'][i] = (out_shapes['mix'][i]/scaler)\n",
    "        out_shapes['num_sing'][i] = len(randsources_with_part)\n",
    "\n",
    "    # if debug == True, pick one random mix from batch and synthesize it\n",
    "    if debug==True:\n",
    "\n",
    "        rand_pick = random.randint(0,batch_size-1)\n",
    "        debug_dir = './debug/it#'+str(itCounter)+'_batchpick#'+str(rand_pick)\n",
    "        if not os.path.isdir(debug_dir):\n",
    "            os.mkdir(debug_dir)\n",
    "        soundfile.write(debug_dir+'/'+'mix_'+str(out_shapes['num_sing'][rand_pick])+'_singers_'+'.wav', out_shapes['mix'][rand_pick], config['resampling_fs'], 'PCM_24')\n",
    "            \n",
    "    \n",
    "    return (out_shapes['mix'], out_shapes['num_sing'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the generator function returns a dictionary of shape:<br>\n",
    "```out_shapes = {'mix':np.zeros((batch_size, num_frames)),'num_sing':np.zeros((batch_size))}``` <br><br>where:\n",
    "* the key ```'mix'``` holds ```<batch_size>``` number of snippet of size ```<num_frames>```\n",
    "* the key ```'num_sing'``` holds the ground truth respective to each of these mixes\n",
    "\n",
    "<br>The purpose of the generator function is to strictly provide the time-domain snippets. For the pre-processing steps related to frequency-domain conversion and more, see the next section below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<h1><center>Features Extraction Step</center></h1>\n",
    "\n",
    "The first step of our experiment entails training a deep neural network model using some low-level spectral features extracted using the audio analysis library **Essentia**. \n",
    "\n",
    "***\n",
    "### 1. Computing Time-Domain Snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frames = config['num_frames'] # size of song snippets\n",
    "\n",
    "audio_data_train, y_train = batchGenerator(config['hdf5_filepath'], config['batch_size_train'], num_frames, partition='train', debug=True)\n",
    "print('Audio Data for Training Loaded with Shape:\\n'+ str(np.shape(audio_data_train)))\n",
    "print('Ground Truth for Training Loaded with Shape:\\n'+ str(np.shape(y_train)))\n",
    "audio_data_test, y_test = batchGenerator(config['hdf5_filepath'], config['batch_size_test'], num_frames, partition='test', debug=True)\n",
    "print('Audio Data for Testing Loaded with Shape:\\n'+ str(np.shape(audio_data_test)))\n",
    "print('Ground Truth for Testing Loaded with Shape:\\n'+ str(np.shape(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our audio data are loaded into a 2D numpy array called ```audio_data``` and our ground truth is loaded onto a 1D numpy array called ```y```. <br>\n",
    "\n",
    "For what it is worth, let's plot a few audio snippets labeled with their respective ground truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(10,100))\n",
    "plt.subplots(4,1,figsize=(20,30))\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "for i in range(4):\n",
    "    \n",
    "    rand_snippet = random.randint(0,np.shape(audio_data_train)[0]-1) # Pick random snippet from audio_data\n",
    "    x = audio_data_train[rand_snippet]\n",
    "    num_singer = y_train[rand_snippet]\n",
    "\n",
    "    plt.subplot(4, 1, (i+1))\n",
    "    plt.plot(x)\n",
    "    plt.title(r\"$\\bf{\"\"Singers: \"+str(num_singer)+ \"}$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 2. Feature Extraction\n",
    "\n",
    "Now is the time to actually extract the features from all the audio snippets we've just computed. We will exploit the fact that each singer in a SATB recording settings each have their respective singing range (that is to say, there is still a fair amount of harmonic overlap). <br>A first naive approach, would be to compute the __MFCC coefficients__ in the hope that this feature would accurately depict the recordings' frequency range in respect to the number of singers singing. Moreover, this feature seems like a good fit for our task as the signals we are dealing with only entail singing voice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from essentia.standard import *\n",
    "\n",
    "x = essentia.array(audio_data_train[0])\n",
    "MonoWriter(filename ='./temp.wav',sampleRate=22050)(x)\n",
    "features, features_frames = MusicExtractor(lowlevelSilentFrames='drop',\n",
    "                                                  lowlevelFrameSize = 2048,\n",
    "                                                  lowlevelHopSize = 1024,\n",
    "                                                  lowlevelStats = ['mean', 'stdev'])('./temp.wav')\n",
    "scalar_lowlevel_descriptors = [descriptor for descriptor in features.descriptorNames() if 'lowlevel' in descriptor and isinstance(features[descriptor], float)]\n",
    "print(\"Subset of features to be considered:\\n\",scalar_lowlevel_descriptors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDataForPartition(audio):\n",
    "    \n",
    "    w = Windowing(type = 'hann')\n",
    "    spectrum = Spectrum(size=4096)\n",
    "    mfcc = MFCC(numberCoefficients=13,numberBands=64,inputSize=4096,sampleRate=config[\"resampling_fs\"])\n",
    "    logNorm = UnaryOperator(type='log')\n",
    "\n",
    "    mfccs = np.empty([np.shape(audio)[0],int((config['num_frames']/512)),13])\n",
    "    melbands_log = np.empty([np.shape(audio)[0],int((config['num_frames']/512)),64])\n",
    "\n",
    "    sleep(1.0)\n",
    "\n",
    "    # Compute MFCCS for all snippets: shape <snippet, coefficients>\n",
    "    total_features = []\n",
    "    for idx in tqdm(range(np.shape(audio)[0])):\n",
    "        \n",
    "        x = audio[idx]\n",
    "        \n",
    "        mfccs_tmp = []\n",
    "        mel_bands_tmp = []\n",
    "        spc_frame = []\n",
    "        for frame in FrameGenerator(essentia.array(x), frameSize=1024, hopSize=512, startFromZero=True):\n",
    "            mfcc_bands, mfcc_coeffs = mfcc(spectrum(w(frame)))\n",
    "            mfccs_tmp.append(mfcc_coeffs)\n",
    "            mel_bands_tmp.append(logNorm(mfcc_bands))\n",
    "\n",
    "        #Calculate average of MFCCs for all frame and store\n",
    "        mfccs[idx,:,:] = mfccs_tmp\n",
    "        melbands_log[idx,:,:] = mel_bands_tmp\n",
    "        \n",
    "    return melbands_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we compute MFCCs for all audio snippet, let's plot some of them to make sure they look as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Computing MFCCS for training set:')\n",
    "X_train = createDataForPartition(audio_data_train)\n",
    "print('Computing MFCCS for test set:')\n",
    "X_test  = createDataForPartition(audio_data_test)\n",
    "\n",
    "plt.figure(1, figsize=(100,100))\n",
    "plt.subplots(4,4,figsize=(20,20))\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "for i in range(16):\n",
    "    \n",
    "    rand_snippet = random.randint(0,np.shape(X_train)[0]-1) # Pick random snippet from audio_data\n",
    "    mfcc = X_train[rand_snippet].T\n",
    "    num_singer = int(y_train[rand_snippet])\n",
    "\n",
    "    plt.subplot(4, 4, (i+1))\n",
    "    plt.imshow(mfcc, aspect='auto', origin='lower', interpolation='none')\n",
    "    plt.title(r\"$\\bf{\"\"Singers: \"+str(num_singer)+ \"}$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def retrieveDataFromCSV(data_path):\n",
    "    #Read data\n",
    "    data = pd.read_csv(data_path)\n",
    "    #Let's see the first lines of our data\n",
    "    data.head()\n",
    "    #sns.pairplot(data.iloc[:, -7:], hue = \"num_sing\");\n",
    "\n",
    "    data_modif = data.copy()\n",
    "\n",
    "    #min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    #data_modif.iloc[:,:84] = min_max_scaler.fit_transform(data.iloc[:,:84].values)\n",
    "    data_modif.describe()\n",
    "    data_modif.num_sing.value_counts()\n",
    "\n",
    "    #input values put in a matrix, there are 84 features\n",
    "    X = data_modif.iloc[:,:84].values \n",
    "    #Creating output values\n",
    "    y = np.array(data_modif.num_sing) #create label encoded outputs\n",
    "    \n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    y_onehot = onehot_encoder.fit_transform(y.reshape(len(y), 1))\n",
    "    \n",
    "    return X, y_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Couple of modules for wrapped feature selection\n",
    "# from sklearn.ensemble import RandomForestClassifier # wrapper-based\n",
    "# from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# X_train, y_train = retrieveDataFromCSV('train_data.csv')\n",
    "# X_test, y_test   = retrieveDataFromCSV('test_data.csv')\n",
    "\n",
    "X_train = np.mean(X_train,axis=1)\n",
    "X_test = np.mean(X_test,axis=1)\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "y_train_onehot = onehot_encoder.fit_transform(y_train.reshape(len(y_train), 1))\n",
    "y_test_onehot = onehot_encoder.fit_transform(y_test.reshape(len(y_test), 1))\n",
    "\n",
    "# Finally, we end up with these shapes:\n",
    "print('Data for Training with Shape:\\n'+ str(np.shape(X_train)))\n",
    "print('Ground Truth for Training Loaded with Shape:\\n'+ str(np.shape(y_train_onehot)))\n",
    "print('Data for Testing with Shape:\\n'+ str(np.shape(X_test)))\n",
    "print('Ground Truth for Testing Loaded with Shape:\\n'+ str(np.shape(y_test_onehot)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "\n",
    "def compose_model(num_features):\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    # Input layer \n",
    "    model.add(layers.BatchNormalization(name='InputLayer', input_shape=(num_features,)))\n",
    "    \n",
    "    # 1. hidden layer\n",
    "    model.add(layers.Dense(name='HiddenLayer_1', units = 20))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Activation('tanh'))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    # 2. hidden layyer\n",
    "    model.add(layers.Dense(name='HiddenLayer_2', units = 40))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Activation('relu'))\n",
    "    \n",
    "    # 3. hidden layyer\n",
    "    model.add(layers.Dense(name='HiddenLayer_3', units = 20))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Activation('relu'))\n",
    "\n",
    "\n",
    "    # Output layer\n",
    "    model.add(layers.Dense(name='Output_layer', units = 4))\n",
    "    model.add(layers.Activation('sigmoid'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = compose_model(X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model.summary()\n",
    "model.compile(optimizer = 'Adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "summary = model.fit(X_train, y_train_onehot, batch_size = 25, epochs = 100, validation_split=0.2, verbose=1)\n",
    "\n",
    "score = model.evaluate(X_test, y_test_onehot, verbose = 1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(summary.history['accuracy'])\n",
    "plt.plot(summary.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='lower right')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(summary.history['loss'])\n",
    "plt.plot(summary.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<h1><center>HCQT Approach</center></h1>\n",
    "\n",
    "Now that we have computed our baseline, which as you could see left room for great improvement, we can proceed in digging into the core of this experiment: __using a deep salience approach to estimate the number of singing sources__. \n",
    "\n",
    "Our experiment will heavily rely on the experiment conducted by Bittner, R et. al tackling the issue of multi-F0 estimation in polyphonic music, using Deep Neural Networks (see: *Deep Salience Representations for F0 Estimation in Polyphonic Music - https://bmcfee.github.io/papers/ismir2017_salience.pdf*).\n",
    "\n",
    "In fact, we will use the companion code presented in the above-mentioned paper in order to estimate the number of simultaneous singing sources: https://github.com/rabitt/ismir2017-deepsalience\n",
    "\n",
    "***\n",
    "### 1. Importing Required Python File - Defining Function Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file works as a standalone processor (no need for other file dependencies)\n",
    "import predict_on_audio as ds\n",
    "from collections import Counter\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset our training/test array\n",
    "X_train = []\n",
    "X_test = []\n",
    "\n",
    "print('Computing HCQT Features for train set:')\n",
    "\n",
    "sleep(2.0)\n",
    "\n",
    "# Compute HCQT for training set\n",
    "for idx in tqdm(range(np.shape(audio_data_train)[0])):\n",
    "    x = audio_data_train[idx]\n",
    "    log_hcqt, freq_grid, time_grid = ds.compute_hcqt(x)\n",
    "    X_train.append(np.mean(log_hcqt,axis=2))\n",
    "\n",
    "print('Computing HCQT Features for test set:')\n",
    "\n",
    "sleep(1.0)\n",
    "\n",
    "# Compute HCQT for test set    \n",
    "for idx in tqdm(range(np.shape(audio_data_test)[0])):\n",
    "    x = audio_data_test[idx]\n",
    "    log_hcqt, freq_grid, time_grid = ds.compute_hcqt(x)\n",
    "    X_test.append(np.mean(log_hcqt,axis=2))\n",
    "\n",
    "# After extractions, we end up with these shapes:\n",
    "print('Data for Training with Shape:\\n'+ str(np.shape(X_train)))\n",
    "print('Ground Truth for Training Loaded with Shape:\\n'+ str(np.shape(y_train)))\n",
    "print('Data for Testing with Shape:\\n'+ str(np.shape(X_test)))\n",
    "print('Ground Truth for Testing Loaded with Shape:\\n'+ str(np.shape(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa.display\n",
    "\n",
    "plt.figure(1, figsize=(100,100))\n",
    "plt.subplots(4,4,figsize=(20,20))\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "for i in range(16):\n",
    "    \n",
    "    rand_snippet  = random.randint(0,np.shape(X_train)[0]-1) # Pick random snippet from audio_data\n",
    "    rand_harmonic = random.randint(0,np.shape(X_train)[1]-1) # Pick random snippet from audio_data\n",
    "    cqt_spec = X_train[rand_snippet][rand_harmonic]\n",
    "    num_singer = int(y_train[rand_snippet])\n",
    "\n",
    "    plt.subplot(4, 4, (i+1))\n",
    "    plt.imshow(cqt_spec, aspect='auto', origin='lower', interpolation='none')\n",
    "    plt.title(r\"$\\bf{\"\"Singers: \"+str(num_singer)+ \"\\nHarmonic#: \"+str(rand_harmonic)+ \"}$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the mean of the third axis (freq)\n",
    "X_train = np.asarray(X_train, dtype=np.float32)\n",
    "X_train_mean = np.mean(X_train,axis=3)\n",
    "X_test = np.asarray(X_test, dtype=np.float32)\n",
    "X_test_mean = np.mean(X_test,axis=3)\n",
    "\n",
    "print('Data for Training with Shape:\\n'+ str(np.shape(X_train_mean)))\n",
    "print('Ground Truth for Training Loaded with Shape:\\n'+ str(np.shape(y_train)))\n",
    "print('Data for Testing with Shape:\\n'+ str(np.shape(X_test_mean)))\n",
    "print('Ground Truth for Testing Loaded with Shape:\\n'+ str(np.shape(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, TimeDistributed, LSTM, Dropout, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten\n",
    "from keras import optimizers\n",
    "\n",
    "def compose_model_2d(feat_dim_1,feat_dim_2,channel):\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(32, kernel_size=(2, 2), activation='relu', input_shape=(feat_dim_1, feat_dim_2, channel)))\n",
    "    model.add(Conv2D(48, kernel_size=(2, 2), activation='relu'))\n",
    "    model.add(Conv2D(120, kernel_size=(2, 2), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Define model input dimensions\n",
    "dim_1   = X_train.shape[1]\n",
    "dim_2   = X_train.shape[2]\n",
    "channel = 1\n",
    "\n",
    "# Get the model\n",
    "model   = compose_model_2d(dim_1,dim_2,channel)\n",
    "\n",
    "# Reshaping to perform 2D convolution\n",
    "X_train_rs = X_train.reshape(X_train.shape[0], dim_1, dim_2, channel)\n",
    "X_test_rs = X_test.reshape(X_test.shape[0], dim_1, dim_2, channel)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "optimizer = optimizers.SGD(lr=0.002, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "summary = model.fit(X_train_rs, y_train_onehot, batch_size = 25, epochs = 40, validation_split=0.2, verbose=1)\n",
    "\n",
    "score = model.evaluate(X_test_rs, y_test_onehot, verbose = 1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(summary.history['accuracy'])\n",
    "plt.plot(summary.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='lower right')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(summary.history['loss'])\n",
    "plt.plot(summary.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
