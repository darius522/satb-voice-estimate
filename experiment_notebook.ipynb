{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We opt to define all the global configuration settings through this dict\n",
    "config = {\"resampling_fs\" : 22050, #expected resampling rate on HDF5 file creation\n",
    "          \"dst_path\":\"./choral_dst\", # User Dataset path (if None, the default one will be used)\n",
    "          \"hdf5_filepath\":\"./choral_dst.hdf5\",\n",
    "          \"batch_size_train\": 512,\n",
    "          \"batch_size_test\": 256,\n",
    "          \"num_frames\": 132300 # 8 sec @ 22050Hz\n",
    "         }\n",
    "\n",
    "if not os.path.isdir('./debug'):\n",
    "    os.mkdir('./debug')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Data Handling Methodology</center></h1>\n",
    "\n",
    "For the first step of this task, we will work from the __Choral Singing Dataset__, which contains three choir songs composed of 4 groups of 4 singers (16 parts per song), for a total of 48 stems. The dataset can be downloaded from Zenodo at the following:\n",
    "\n",
    "https://zenodo.org/record/2649950#.XlJRNy2ZPRY\n",
    "\n",
    "If you happen to already have the dataset locally, please define it's local path through the ***dst_path*** variable. Otherwise, the cell below will take care of downloading it for you:\n",
    "***\n",
    "### 1. Dataset Download:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell downloads the Mridangam Stroke Dataset in its entirety\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import os, sys,shutil\n",
    "\n",
    "#Link for the dataset. If you already have the dataset locally, define its location through the config dict above \n",
    "url = 'https://zenodo.org/record/2649950/files/ChoralSingingDataset.zip'\n",
    "\n",
    "if config[\"dst_path\"] == None:\n",
    "    main_data_dir = './choral_dst'\n",
    "    if not os.path.exists(main_data_dir): #creating the directory if not exist\n",
    "        os.mkdir(main_data_dir)\n",
    "    print('Downloading Choral Singing Dataset Dataset...')\n",
    "    foldername = url.split('/')[-1]\n",
    "    urllib.request.urlretrieve(url,foldername)\n",
    "    #Unzipping to a specific folder\n",
    "    zip_ref = zipfile.ZipFile(foldername, 'r')\n",
    "    zip_ref.extractall(main_data_dir)\n",
    "    zip_ref.close()\n",
    "    os.remove(foldername)#Removing the zip file\n",
    "    print('Data downloaded and unzipped to: ',main_data_dir)\n",
    "else:\n",
    "    print('User Path Defined. Dataset Expected to be Found at the Following:\\n'+config[\"dst_path\"])\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 2. Storing our Data in a Convenient Format\n",
    "\n",
    "For the sake of simplicity and clarity, we will store our song stems in a **HDF5 file**, which will help us to manage the later steps a bit more easily. This hierarchical file format will allow us to store our data in an intuitive way as follow:\n",
    "\n",
    "**[HDF5_File] -> [Set] -> [Song] -> [Part]**\n",
    "\n",
    "Since our dataset is of *very* limited size, we will break it down as follow:\n",
    "\n",
    "| Set   | Stems | Songs |\n",
    "| :---:   | :---: | :---: |\n",
    "| Train |    24   |    A, B   |\n",
    "| Valid |   8    |   A, B    |\n",
    "| Test  |16| C |\n",
    "\n",
    "**Note:** By default, the Choral Singing Dataset follow the following naming convention:\n",
    "\n",
    "*DatasetName_SongAcronym_PartName_PartNumber.wav*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import librosa\n",
    "import soundfile\n",
    "import glob\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "from pathlib import Path\n",
    "\n",
    "# First check if HDF5 already exists:\n",
    "hdf5_file = Path(config['hdf5_filepath'])\n",
    "if not hdf5_file.is_file():\n",
    "    tracks = dict()\n",
    "    resampling_fs = config['resampling_fs']\n",
    "    dst_path = './choral_dst' if (config['dst_path'] == None) else config['dst_path']\n",
    "\n",
    "    # Get all the stem files\n",
    "    dst_stems = glob.glob(dst_path+\"/*.wav\",recursive=False)\n",
    "    # Extract only songs from name\n",
    "    songs = set([os.path.splitext(os.path.basename(i))[0].split('_')[1] for i in dst_stems])\n",
    "    # Take 2/3 of songs for train/valid\n",
    "    train_val_songs = random.sample(songs, int(np.ceil(0.666*len(songs))))\n",
    "    # Take the rest as test set\n",
    "    test_song = list(set(songs) - set(train_val_songs))\n",
    "    test_idx = [int(i) for i, track in enumerate(dst_stems) if [True for song in test_song if str('_'+song+'_') in track]]\n",
    "    # Take one random group of singer per song for each song as validation set\n",
    "    rand_part = random.randint(1,4)\n",
    "    val_idx   = [int(i) for i, track in enumerate(dst_stems) if [True for song in train_val_songs if ((str('_'+song+'_') in track) and (str('_'+str(rand_part)+'.wav')) in track)]]\n",
    "    # Deduce the train set from valid / test set\n",
    "    train_idx = list(set(range(len(dst_stems))) -  (set(val_idx + test_idx)))\n",
    "\n",
    "    # tracks['train'] = np.take(dst_stems, train_idx)\n",
    "    tracks['train'] = np.take(dst_stems, train_idx)\n",
    "    tracks['valid'] = np.take(dst_stems, val_idx)\n",
    "    tracks['test'] = np.take(dst_stems, test_idx)\n",
    "\n",
    "    print('Ready to create HDF5')\n",
    "\n",
    "    h5file = h5py.File(config[\"hdf5_filepath\"], \"w\")\n",
    "\n",
    "    # Write stems to hdf5 file for train/valid/test partitions\n",
    "    for curr_partition in [\"train\", \"valid\", \"test\"]:\n",
    "\n",
    "        print(\"Writing \" + curr_partition + \" partition with \"+str(len(tracks[curr_partition]))+\" files...\")\n",
    "\n",
    "        stem_list = tracks[curr_partition]\n",
    "\n",
    "        # Create group for set if needed\n",
    "        if not str(curr_partition) in h5file:\n",
    "            set_grp  = h5file.create_group(curr_partition)\n",
    "\n",
    "        sleep(1)\n",
    "\n",
    "        for track in tqdm(stem_list):\n",
    "\n",
    "            filename = os.path.splitext(os.path.basename(track))[0].split('_')\n",
    "\n",
    "            song = filename[1]\n",
    "            part = ''.join(filename[2:4])\n",
    "\n",
    "            # Create group for the Song if needed\n",
    "            if not str(curr_partition+'/'+song) in h5file:\n",
    "                song_grp  = set_grp.create_group(song)\n",
    "\n",
    "            # Create group for the Part if needed\n",
    "            if not str(curr_partition+'/'+song+'/'+part) in h5file:\n",
    "                part_grp = h5file[str(curr_partition+'/'+song)]\n",
    "                subgrp  = part_grp.create_group(part)\n",
    "\n",
    "            # Once part groups / song subgroups are created, store file\n",
    "            audio, s = librosa.load(track, sr=resampling_fs)\n",
    "            subgrp = h5file[str(curr_partition+'/'+song+'/'+part)]\n",
    "            subgrp.create_dataset(\"raw_wav\",data=audio)\n",
    "\n",
    "    print('Done Creating HDF5 at path: '+config['hdf5_filepath'])\n",
    "    h5file.close()\n",
    "else:\n",
    "    print('HDF5 Already Exists. File Expected to be Found at the Following:\\n'+config[\"hdf5_filepath\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 3. Defining the Generator Function:\n",
    "\n",
    "Instead of computing features on the entirety of each of these songs, we will create many song snippets (or frames), which will depict different possible musical settings (SAT, ATB, SB, etc.) out of the songs. This will allow us to cover all possible singer permutations on a per-frame basis, without having to compute and feed to the model substantial chunk of audio.\n",
    "\n",
    "The next step would be to define the function that will generate these song frames for us. \n",
    "\n",
    "**Note:** This function will yield the song segments but won't render them to disk. We can proceed by computing everything on the fly and save ourselves some local memory space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchGenerator(hdf5_filepath, batch_size, num_frames, partition='train', debug=False):\n",
    "    \n",
    "    dataset   = h5py.File(hdf5_filepath, \"r\")\n",
    "    itCounter = 0\n",
    "\n",
    "    itCounter = itCounter + 1\n",
    "    \n",
    "    # Choose random song and retrieve all available part for it\n",
    "    randsong = random.choice(list(dataset[partition].keys())) # Get random song from dataset keys\n",
    "    sources  = list(dataset[partition][randsong].keys())      # Get all available sources for song\n",
    "    \n",
    "    # Compute available parts per group\n",
    "    part_per_group = dict({'soprano':None,'alto':None,'tenor':None,'bass':None})\n",
    "    for group in part_per_group.keys():\n",
    "        parts = [source[-1] for source in sources if group in source]\n",
    "        part_per_group[group] = parts\n",
    "\n",
    "    # Init the start/end values of song snippet\n",
    "    startspl = 0\n",
    "    endspl   = 0\n",
    "\n",
    "    # Preping the output shape of the generator\n",
    "    out_shapes = {'mix':np.zeros((batch_size, num_frames)),'num_sing':np.zeros((batch_size),dtype=int)}\n",
    "\n",
    "    # Will create batch_size x mixes of unique singers permutations\n",
    "    for i in range(batch_size):\n",
    "\n",
    "        randsources = random.sample(part_per_group.keys(), random.randint(1,4)) # Randomize source pick\n",
    "\n",
    "        # Get Start and End samples. Pick random part to calculate start/end spl\n",
    "        randpart = random.choice(sources)\n",
    "        startspl = random.randint(0,len(dataset[partition][randsong][randpart]['raw_wav'])-num_frames) # This assume that all stems are the same length\n",
    "        endspl   = startspl+num_frames\n",
    "\n",
    "        # Get Random Sources with part number: \n",
    "        randsources_with_part = [\"{}{}\".format(a, random.choice(part_per_group[a])) for a in randsources] # Concatenate strings for part name\n",
    "        # Retrieve the chunks and store them in output shapes                                         \n",
    "        for source in randsources_with_part:\n",
    "\n",
    "            source_chunk = dataset[partition][randsong][source]['raw_wav'][startspl:endspl]              # Retrieve part's chunk\n",
    "            out_shapes['mix'][i] = np.add(out_shapes['mix'][i],source_chunk)            # Add the chunk to the mix\n",
    "\n",
    "        # Scale down the mix based off number of sources\n",
    "        scaler = len(randsources_with_part)\n",
    "        out_shapes['mix'][i] = (out_shapes['mix'][i]/scaler)\n",
    "        out_shapes['num_sing'][i] = len(randsources_with_part)\n",
    "\n",
    "    # if debug == True, pick one random mix from batch and synthesize it\n",
    "    if debug==True:\n",
    "\n",
    "        rand_pick = random.randint(0,batch_size-1)\n",
    "        debug_dir = './debug/it#'+str(itCounter)+'_batchpick#'+str(rand_pick)\n",
    "        if not os.path.isdir(debug_dir):\n",
    "            os.mkdir(debug_dir)\n",
    "        soundfile.write(debug_dir+'/'+'mix_'+str(out_shapes['num_sing'][rand_pick])+'_singers_'+'.wav', out_shapes['mix'][rand_pick], config['resampling_fs'], 'PCM_24')\n",
    "            \n",
    "    \n",
    "    return (out_shapes['mix'], out_shapes['num_sing'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the generator function returns a dictionary of shape:<br>\n",
    "```out_shapes = {'mix':np.zeros((batch_size, num_frames)),'num_sing':np.zeros((batch_size))}``` <br><br>where:\n",
    "* the key ```'mix'``` holds ```<batch_size>``` number of snippet of size ```<num_frames>```\n",
    "* the key ```'num_sing'``` holds the ground truth respective to each of these mixes\n",
    "\n",
    "<br>The purpose of the generator function is to strictly provide the time-domain snippets. For the pre-processing steps related to frequency-domain conversion and more, see the next section below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<h1><center>Features Extraction Step</center></h1>\n",
    "\n",
    "The first step of our experiment entails training a deep neural network model using some low-level spectral features extracted using the audio analysis library **Essentia**. \n",
    "\n",
    "***\n",
    "### 1. Computing Time-Domain Snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frames = config['num_frames'] # size of song snippets\n",
    "\n",
    "audio_data_train, y_train = batchGenerator(config['hdf5_filepath'], config['batch_size_train'], num_frames, partition='train', debug=True)\n",
    "print('Audio Data for Training Loaded with Shape:\\n'+ str(np.shape(audio_data_train)))\n",
    "print('Ground Truth for Training Loaded with Shape:\\n'+ str(np.shape(y_train)))\n",
    "audio_data_test, y_test = batchGenerator(config['hdf5_filepath'], config['batch_size_test'], num_frames, partition='test', debug=True)\n",
    "print('Audio Data for Testing Loaded with Shape:\\n'+ str(np.shape(audio_data_test)))\n",
    "print('Ground Truth for Testing Loaded with Shape:\\n'+ str(np.shape(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our audio data are loaded into a 2D numpy array called ```audio_data``` and our ground truth is loaded onto a 1D numpy array called ```y```. <br>\n",
    "\n",
    "For what it is worth, let's plot a few audio snippets labeled with their respective ground truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(10,100))\n",
    "plt.subplots(4,1,figsize=(20,30))\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "for i in range(4):\n",
    "    \n",
    "    rand_snippet = random.randint(0,np.shape(audio_data_train)[0]-1) # Pick random snippet from audio_data\n",
    "    x = audio_data_train[rand_snippet]\n",
    "    num_singer = y_train[rand_snippet]\n",
    "\n",
    "    plt.subplot(4, 1, (i+1))\n",
    "    plt.plot(x)\n",
    "    plt.title(r\"$\\bf{\"\"Singers: \"+str(num_singer)+ \"}$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 2. Feature Extraction\n",
    "\n",
    "Now is the time to actually extract the features from all the audio snippets we've just computed. We will exploit the fact that each singer in a SATB recording settings each have their respective singing range (that is to say, there is still a fair amount of harmonic overlap). <br>A first naive approach, would be to compute the __MFCC coefficients__ in the hope that this feature would accurately depict the recordings' frequency range in respect to the number of singers singing. Moreover, this feature seems like a good fit for our task as the signals we are dealing with only entail singing voice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from essentia.standard import *\n",
    "\n",
    "x = essentia.array(audio_data_train[0])\n",
    "MonoWriter(filename ='./webgwegweg.wav',sampleRate=22050)(x)\n",
    "features, features_frames = MusicExtractor(lowlevelSilentFrames='drop',\n",
    "                                                  lowlevelFrameSize = 2048,\n",
    "                                                  lowlevelHopSize = 1024,\n",
    "                                                  lowlevelStats = ['mean', 'stdev'])('./webgwegweg.wav')\n",
    "scalar_lowlevel_descriptors = [descriptor for descriptor in features.descriptorNames() if 'lowlevel' in descriptor and isinstance(features[descriptor], float)]\n",
    "print(\"Subset of features to be considered:\\n\",scalar_lowlevel_descriptors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDataForPartition(audio,y,partition='train'):\n",
    "    \n",
    "    w = Windowing(type = 'hann')\n",
    "    spectrum = Spectrum()\n",
    "    mfcc = MFCC(numberCoefficients=13)\n",
    "    logNorm = UnaryOperator(type='log')\n",
    "\n",
    "    mfccs = np.empty([np.shape(audio)[0],int((config['num_frames']/512)),13])\n",
    "    melbands_log = np.empty([np.shape(audio)[0],int((config['num_frames']/512)),40])\n",
    "\n",
    "    sleep(1.0)\n",
    "\n",
    "    # Compute MFCCS for all snippets: shape <snippet, coefficients>\n",
    "    total_features = []\n",
    "    data_file = os.path.join('./',partition+'_data.csv')\n",
    "    with open(data_file, 'w') as writer:\n",
    "        \n",
    "        #adding column names as the first line in csv\n",
    "        line2write = ','.join(scalar_lowlevel_descriptors + ['num_sing']).replace('lowlevel.','') + '\\n'\n",
    "        writer.write(line2write)\n",
    "        \n",
    "        for idx in tqdm(range(np.shape(audio)[0])):\n",
    "\n",
    "            x = essentia.array(audio[idx])\n",
    "            MonoWriter(filename ='./webgwegweg.wav',sampleRate=22050)(x)\n",
    "\n",
    "            try:\n",
    "                features, features_frames = MusicExtractor(lowlevelSilentFrames='drop',\n",
    "                                                          lowlevelFrameSize = 2048,\n",
    "                                                          lowlevelHopSize = 1024,\n",
    "                                                          lowlevelStats = ['mean', 'stdev'])('./webgwegweg.wav')\n",
    "                selected_features = [features[descriptor] for descriptor in scalar_lowlevel_descriptors]\n",
    "                total_features.append(selected_features)\n",
    "                line2write = str(selected_features)[1:-1] + ',' + str(y[idx]) + '\\n'\n",
    "                writer.write(line2write)\n",
    "            except:\n",
    "                audio[idx] = 0\n",
    "                continue\n",
    "            os.remove('./webgwegweg.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we compute MFCCs for all audio snippet, let's plot some of them to make sure they look as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Computing Low Level Features for training set:')\n",
    "createDataForPartition(audio_data_train,y_train,partition='train')\n",
    "print('Computing Low Level Features for test set:')\n",
    "createDataForPartition(audio_data_test,y_test,partition='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def retrieveDataFromCSV(data_path):\n",
    "    #Read data\n",
    "    data = pd.read_csv(data_path)\n",
    "    #Let's see the first lines of our data\n",
    "    data.head()\n",
    "    #sns.pairplot(data.iloc[:, -7:], hue = \"num_sing\");\n",
    "\n",
    "    data_modif = data.copy()\n",
    "\n",
    "    #min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    #data_modif.iloc[:,:84] = min_max_scaler.fit_transform(data.iloc[:,:84].values)\n",
    "    data_modif.describe()\n",
    "    data_modif.num_sing.value_counts()\n",
    "\n",
    "    #input values put in a matrix, there are 84 features\n",
    "    X = data_modif.iloc[:,:84].values \n",
    "    #Creating output values\n",
    "    y = np.array(data_modif.num_sing) #create label encoded outputs\n",
    "    \n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    y_onehot = onehot_encoder.fit_transform(y.reshape(len(y), 1))\n",
    "    \n",
    "    return X, y_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Couple of modules for wrapped feature selection\n",
    "from sklearn.ensemble import RandomForestClassifier # wrapper-based\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "X_train, y_train = retrieveDataFromCSV('train_data.csv')\n",
    "X_test, y_test   = retrieveDataFromCSV('test_data.csv')\n",
    "\n",
    "# Finally, we end up with these shapes:\n",
    "print('Pre Selection Audio Data for Training with Shape:\\n'+ str(np.shape(X_train)))\n",
    "print('Pre Selection Ground Truth for Training Loaded with Shape:\\n'+ str(np.shape(y_train)))\n",
    "print('Pre Selection Audio Data for Testing with Shape:\\n'+ str(np.shape(X_test)))\n",
    "print('Pre Selection Ground Truth for Testing Loaded with Shape:\\n'+ str(np.shape(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "\n",
    "def compose_model(num_features):\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    # Input layer \n",
    "    model.add(layers.BatchNormalization(name='InputLayer', input_shape=(num_features,)))\n",
    "    \n",
    "    # 1. hidden layer\n",
    "    model.add(layers.Dense(name='HiddenLayer_1', units = 40))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Activation('tanh'))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    # 2. hidden layyer\n",
    "    model.add(layers.Dense(name='HiddenLayer_2', units = 20))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(layers.Dense(name='Output_layer', units = 4))\n",
    "    model.add(layers.Activation('sigmoid'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = compose_model(X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model.summary()\n",
    "model.compile(optimizer = 'Adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "summary = model.fit(X_train, y_train, batch_size = 50, epochs = 250, validation_split=0.2, verbose=0,)\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose = 0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<h1><center>Deep Salience Approach</center></h1>\n",
    "\n",
    "Now that we have computed our baseline, which as you could see left room for great improvement, we can proceed in digging into the core of this experiment: __using a deep salience approach to estimate the number of singing sources__. \n",
    "\n",
    "Our experiment will heavily rely on the experiment conducted by Bittner, R et. al tackling the issue of multi-F0 estimation in polyphonic music, using Deep Neural Networks (see: *Deep Salience Representations for F0 Estimation in Polyphonic Music - https://bmcfee.github.io/papers/ismir2017_salience.pdf*).\n",
    "\n",
    "In fact, we will use the companion code presented in the above-mentioned paper in order to estimate the number of simultaneous singing sources: https://github.com/rabitt/ismir2017-deepsalience\n",
    "\n",
    "***\n",
    "### 1. Importing Required Python File - Defining Function Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# This file works as a standalone processor (no need for other file dependencies)\n",
    "import predict_on_audio as ds\n",
    "from collections import Counter\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arg Dict passed to the DeepSalience main function\n",
    "model_path = './weights/model.ckpt'\n",
    "args = {'audio':None, # audio chunk np array\n",
    "        'task':'multif0', # \"all, bass, melody1, melody2, melody3, multif0, pitch, vocal.\"\n",
    "        'save_dir':'./', # no used\n",
    "        'use_neg':True, \n",
    "        'threshold':0.7, # IMPORTANT\n",
    "        'output_format':'multif0', # ['singlef0', 'multif0', 'salience']\n",
    "        'model':None} # Hold the deep salience model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./weights/model.ckpt-4590\n",
      "WARNING:tensorflow:From <ipython-input-3-fc0fd2e27d06>:16: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow_core/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From <ipython-input-3-fc0fd2e27d06>:18: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "# np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "# audio_data_test_new = audio_data_test[~np.all(audio_data_test == 0, axis=1)]\n",
    "# y_predict = np.zeros(np.shape(audio_data_test_new)[0])\n",
    "\n",
    "# Preload model \n",
    "init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "sess=tf.Session()\n",
    "sess.run(init_op)\n",
    "#First let's load meta graph and restore weights\n",
    "saver = tf.train.import_meta_graph('./weights/model.ckpt-4590.meta')\n",
    "saver.restore(sess,'./weights/model.ckpt-4590')\n",
    "\n",
    "def DeepSalience(input_, is_train):\n",
    "    conv1 = tf.layers.conv2d(input_, 128, (5, 5), strides=(1, 1), padding='same', name=\"conv_1\", activation = tf.nn.relu)\n",
    "\n",
    "    conv1 = tf.layers.batch_normalization(conv1, training=is_train)\n",
    "\n",
    "    conv2 = tf.layers.conv2d(conv1, 64, (5, 5), strides=(1, 1), padding='same', name=\"conv_2\", activation = tf.nn.relu)\n",
    "\n",
    "    conv2 = tf.layers.batch_normalization(conv2, training=is_train)\n",
    "\n",
    "    conv3 = tf.layers.conv2d(conv2, 64, (3, 3), strides=(1, 1), padding='same', name=\"conv_3\", activation = tf.nn.relu)\n",
    "\n",
    "    conv3 = tf.layers.batch_normalization(conv3, training=is_train)\n",
    "\n",
    "    conv4 = tf.layers.conv2d(conv3, 64, (3, 3), strides=(1, 1), padding='same', name=\"conv_4\", activation = tf.nn.relu)\n",
    "\n",
    "    conv4 = tf.layers.batch_normalization(conv4, training=is_train)\n",
    "\n",
    "    conv5 = tf.layers.conv2d(conv4, 64, (3, 70), strides=(1, 1), padding='same', name=\"conv_5\", activation = tf.nn.relu)\n",
    "\n",
    "    conv5 = tf.layers.batch_normalization(conv5, training=is_train)\n",
    "\n",
    "    final_layer = tf.layers.conv2d(conv5, 1, (1, 1), strides=(1, 1), padding='same', name=\"conv_6\", activation = None)\n",
    "\n",
    "    return tf.squeeze(final_layer)\n",
    "\n",
    "batch_size = 1\n",
    "max_phr_len = 517\n",
    "cqt_bins = 360\n",
    "is_train = False\n",
    "input_placeholder = tf.placeholder(tf.float32, shape=(batch_size, max_phr_len, cqt_bins, 6), name='input_placeholder')\n",
    "output_logits = DeepSalience(input_placeholder, is_train)\n",
    "outputs = tf.nn.sigmoid(output_logits)\n",
    "\n",
    "# for idx in tqdm(range(np.shape(audio_data_test_new)[0])):\n",
    "    \n",
    "#     args['audio'] = audio_data_test[idx]\n",
    "#     times, freqs  = ds.main(args)\n",
    "#     # Counting the simultaneous number of singers from the multi_f0 output\n",
    "#     multi_f0 = np.asarray([len(freqs[idx]) for idx in range(len(freqs)) if (len(freqs[idx])>0)])\n",
    "#     if len(multi_f0)==0:\n",
    "#         multi_f0=[0]\n",
    "#     # Probably better ways to do this, but for now let's stick to it\n",
    "\n",
    "#     #y_predict[idx] = int(np.around(np.mean(multi_f0)))\n",
    "#     y_predict[idx] = Counter(multi_f0).most_common(1)[0][0]\n",
    "\n",
    "# # Make sure that num_singer is within range\n",
    "# y_predict[y_predict < 1] = 1\n",
    "# y_predict[y_predict > 4] = 4\n",
    "# onehot_encoder = OneHotEncoder(sparse=False)\n",
    "# y_predict_onehot = onehot_encoder.fit_transform(y_predict.reshape(len(y_predict), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132300\n",
      "(6, 360, 517)\n"
     ]
    },
    {
     "ename": "FailedPreconditionError",
     "evalue": "Attempting to use uninitialized value conv_1/kernel\n\t [[node conv_1/kernel/read (defined at /usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1748) ]]\n\nOriginal stack trace for 'conv_1/kernel/read':\n  File \"/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.7/site-packages/traitlets/config/application.py\", line 664, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 563, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 148, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py\", line 534, in run_forever\n    self._run_once()\n  File \"/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py\", line 1771, in _run_once\n    handle._run()\n  File \"/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/usr/local/lib/python3.7/site-packages/tornado/ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"/usr/local/lib/python3.7/site-packages/tornado/ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"/usr/local/lib/python3.7/site-packages/tornado/gen.py\", line 787, in inner\n    self.run()\n  File \"/usr/local/lib/python3.7/site-packages/tornado/gen.py\", line 748, in run\n    yielded = self.gen.send(value)\n  File \"/usr/local/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/usr/local/lib/python3.7/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/usr/local/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 272, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/usr/local/lib/python3.7/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/usr/local/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 542, in execute_request\n    user_expressions, allow_stdin,\n  File \"/usr/local/lib/python3.7/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/usr/local/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2855, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/usr/local/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2881, in _run_cell\n    return runner(coro)\n  File \"/usr/local/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/usr/local/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3058, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3249, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"/usr/local/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-3-fc0fd2e27d06>\", line 45, in <module>\n    output_logits = DeepSalience(input_placeholder, is_train)\n  File \"<ipython-input-3-fc0fd2e27d06>\", line 16, in DeepSalience\n    conv1 = tf.layers.conv2d(input_, 128, (5, 5), strides=(1, 1), padding='same', name=\"conv_1\", activation = tf.nn.relu)\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/util/deprecation.py\", line 324, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/layers/convolutional.py\", line 424, in conv2d\n    return layer.apply(inputs)\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/util/deprecation.py\", line 324, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 1700, in apply\n    return self.__call__(inputs, *args, **kwargs)\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/layers/base.py\", line 548, in __call__\n    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 824, in __call__\n    self._maybe_build(inputs)\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 2146, in _maybe_build\n    self.build(input_shapes)\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/convolutional.py\", line 165, in build\n    dtype=self.dtype)\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/layers/base.py\", line 461, in add_weight\n    **kwargs)\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 529, in add_weight\n    aggregation=aggregation)\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py\", line 712, in _add_variable_with_custom_getter\n    **kwargs_for_getter)\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/ops/variable_scope.py\", line 1500, in get_variable\n    aggregation=aggregation)\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/ops/variable_scope.py\", line 1243, in get_variable\n    aggregation=aggregation)\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/ops/variable_scope.py\", line 567, in get_variable\n    aggregation=aggregation)\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/ops/variable_scope.py\", line 519, in _true_getter\n    aggregation=aggregation)\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/ops/variable_scope.py\", line 933, in _get_single_variable\n    aggregation=aggregation)\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py\", line 258, in __call__\n    return cls._variable_v1_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py\", line 219, in _variable_v1_call\n    shape=shape)\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py\", line 197, in <lambda>\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/ops/variable_scope.py\", line 2519, in default_variable_creator\n    shape=shape)\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py\", line 262, in __call__\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py\", line 1688, in __init__\n    shape=shape)\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py\", line 1872, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/util/dispatch.py\", line 180, in wrapper\n    return target(*args, **kwargs)\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py\", line 203, in identity\n    ret = gen_array_ops.identity(input, name=name)\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_array_ops.py\", line 4239, in identity\n    \"Identity\", input=input, name=name)\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py\", line 794, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 3357, in create_op\n    attrs, op_def, compute_device)\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 3426, in _create_op_internal\n    op_def=op_def)\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 1748, in __init__\n    self._traceback = tf_stack.extract_stack()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value conv_1/kernel\n\t [[{{node conv_1/kernel/read}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-db4fae433571>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m#     time_2, est_freq = utils.process_output(out_batches_atb)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mextract_f0_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-db4fae433571>\u001b[0m in \u001b[0;36mextract_f0_file\u001b[0;34m(file_name, sess)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m#for in_batch_hcqt in in_batches_hcqt:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0minput_placeholder\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0min_batches_hcqt\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mout_atb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mout_batches_atb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_atb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mout_batches_atb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_batches_atb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                     \u001b[0;34m'\\nsession_config.graph_options.rewrite_options.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m                     'disable_meta_optimizer = True')\n\u001b[0;32m-> 1384\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value conv_1/kernel\n\t [[node conv_1/kernel/read (defined at /usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1748) ]]\n\nOriginal stack trace for 'conv_1/kernel/read':\n  File \"/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.7/site-packages/traitlets/config/application.py\", line 664, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 563, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 148, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py\", line 534, in run_forever\n    self._run_once()\n  File \"/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py\", line 1771, in _run_once\n    handle._run()\n  File \"/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/usr/local/lib/python3.7/site-packages/tornado/ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"/usr/local/lib/python3.7/site-packages/tornado/ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"/usr/local/lib/python3.7/site-packages/tornado/gen.py\", line 787, in inner\n    self.run()\n  File \"/usr/local/lib/python3.7/site-packages/tornado/gen.py\", line 748, in run\n    yielded = self.gen.send(value)\n  File \"/usr/local/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/usr/local/lib/python3.7/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/usr/local/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 272, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/usr/local/lib/python3.7/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/usr/local/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 542, in execute_request\n    user_expressions, allow_stdin,\n  File \"/usr/local/lib/python3.7/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/usr/local/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2855, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/usr/local/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2881, in _run_cell\n    return runner(coro)\n  File \"/usr/local/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/usr/local/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3058, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3249, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"/usr/local/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-3-fc0fd2e27d06>\", line 45, in <module>\n    output_logits = DeepSalience(input_placeholder, is_train)\n  File \"<ipython-input-3-fc0fd2e27d06>\", line 16, in DeepSalience\n    conv1 = tf.layers.conv2d(input_, 128, (5, 5), strides=(1, 1), padding='same', name=\"conv_1\", activation = tf.nn.relu)\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/util/deprecation.py\", line 324, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/layers/convolutional.py\", line 424, in conv2d\n    return layer.apply(inputs)\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/util/deprecation.py\", line 324, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 1700, in apply\n    return self.__call__(inputs, *args, **kwargs)\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/layers/base.py\", line 548, in __call__\n    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 824, in __call__\n    self._maybe_build(inputs)\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 2146, in _maybe_build\n    self.build(input_shapes)\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/convolutional.py\", line 165, in build\n    dtype=self.dtype)\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/layers/base.py\", line 461, in add_weight\n    **kwargs)\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 529, in add_weight\n    aggregation=aggregation)\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py\", line 712, in _add_variable_with_custom_getter\n    **kwargs_for_getter)\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/ops/variable_scope.py\", line 1500, in get_variable\n    aggregation=aggregation)\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/ops/variable_scope.py\", line 1243, in get_variable\n    aggregation=aggregation)\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/ops/variable_scope.py\", line 567, in get_variable\n    aggregation=aggregation)\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/ops/variable_scope.py\", line 519, in _true_getter\n    aggregation=aggregation)\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/ops/variable_scope.py\", line 933, in _get_single_variable\n    aggregation=aggregation)\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py\", line 258, in __call__\n    return cls._variable_v1_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py\", line 219, in _variable_v1_call\n    shape=shape)\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py\", line 197, in <lambda>\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/ops/variable_scope.py\", line 2519, in default_variable_creator\n    shape=shape)\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py\", line 262, in __call__\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py\", line 1688, in __init__\n    shape=shape)\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py\", line 1872, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/util/dispatch.py\", line 180, in wrapper\n    return target(*args, **kwargs)\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py\", line 203, in identity\n    ret = gen_array_ops.identity(input, name=name)\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_array_ops.py\", line 4239, in identity\n    \"Identity\", input=input, name=name)\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py\", line 794, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 3357, in create_op\n    attrs, op_def, compute_device)\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 3426, in _create_op_internal\n    op_def=op_def)\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 1748, in __init__\n    self._traceback = tf_stack.extract_stack()\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def extract_f0_file(file_name, sess):\n",
    "    \n",
    "    y,sr = librosa.load('./mix_3_singers_.wav', sr=22050)\n",
    "    print(len(y))\n",
    "    in_batches_hcqt, _, nchunks_in = ds.compute_hcqt(y)\n",
    "    print(in_batches_hcqt.shape)\n",
    "    in_batches_hcqt = in_batches_hcqt.reshape(batch_size, max_phr_len, cqt_bins,in_batches_hcqt.shape[0])\n",
    "    #in_batches_hcqt = np.swapaxes(in_batches_hcqt, -1, -2)\n",
    "    out_batches_atb = []\n",
    "    #for in_batch_hcqt in in_batches_hcqt:\n",
    "    feed_dict = {input_placeholder: in_batches_hcqt}\n",
    "    out_atb = sess.run(outputs, feed_dict=feed_dict)\n",
    "    out_batches_atb.append(out_atb)\n",
    "    out_batches_atb = np.array(out_batches_atb)\n",
    "\n",
    "    out_batches_atb = out_batches_atb[:atb.shape[0]]\n",
    "    #     time_2, est_freq = utils.process_output(out_batches_atb)\n",
    "\n",
    "extract_f0_file('./',sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(audio_data_test)[0])\n",
    "print(y_test)\n",
    "print(y_predict_onehot)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_predict_onehot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
